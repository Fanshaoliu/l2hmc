{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils.func_utils import accept, jacobian, autocovariance, get_log_likelihood, get_data, binarize, normal_kl\n",
    "from utils.distributions import Gaussian, GMM, GaussianFunnel, gen_ring\n",
    "from utils.layers import Linear\n",
    "from utils.dynamics import Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size1 = 10\n",
    "size2 = 10\n",
    "\n",
    "class Network(object):\n",
    "    def __init__(self, x_dim, scope='Network', factor=1.0):\n",
    "        with tf.variable_scope(scope):\n",
    "            self.embed_1 = Linear(x_dim, size1, scope='embed_1', factor=1.0 / 3)\n",
    "            self.embed_2 = Linear(x_dim, size1, scope='embed_2', factor=factor * 1.0 / 3)\n",
    "            self.embed_3 = Linear(2, size1, scope='embed_3', factor=1.0 / 3)\n",
    "\n",
    "            self.linear_1 = Linear(size1, size2, scope='linear_1')\n",
    "\n",
    "            self.scaling_S = tf.exp(tf.get_variable('scale_s', shape=(1, x_dim), initializer=tf.constant_initializer(0.)))\n",
    "            self.scaling_F = tf.exp(tf.get_variable('scale_f', shape=(1, x_dim), initializer=tf.constant_initializer(0.)))      \n",
    "            self.linear_s = Linear(size2, x_dim, scope='linear_s', factor=0.001)\n",
    "            self.linear_t = Linear(size2, x_dim, scope='linear_t', factor=0.001)\n",
    "            self.linear_f = Linear(size2, x_dim, scope='linear_f', factor=0.001)\n",
    "      \n",
    "    def hidden(self, x, v, t):\n",
    "        z1 = self.embed_1(x)\n",
    "        z2 = self.embed_2(v)\n",
    "        z3 = self.embed_3(t)\n",
    "\n",
    "        h1 = tf.nn.relu(z1 + z2 + z3)\n",
    "\n",
    "        h2 = tf.nn.relu(self.linear_1(h1))\n",
    "        # h3 = tf.nn.relu(self.linear_2(h2))\n",
    "        return tf.nn.relu(h2)\n",
    "\n",
    "    def S(self, x, v, t, aux=None):\n",
    "        h = self.hidden(x, v, t)\n",
    "        use_tanh = True\n",
    "        if use_tanh:\n",
    "            return self.scaling_S * tf.nn.tanh(self.linear_s(h))\n",
    "        else: \n",
    "            return self.linear_s(h)\n",
    "\n",
    "    def T(self, x, v, t, aux=None):\n",
    "        h = self.hidden(x, v, t)\n",
    "        return self.linear_t(h)\n",
    "\n",
    "    def F(self, x, v, t, aux=None):\n",
    "        h = self.hidden(x, v, t)\n",
    "        return self.scaling_F * tf.nn.tanh(self.linear_f(h))\n",
    "\n",
    "def net_factory(x_dim, scope, factor):\n",
    "    return Network(x_dim, scope=scope, factor=factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def boolean_mask(x, mask):\n",
    "    return tf.boolean_mask(x, mask), tf.boolean_mask(x, tf.logical_not(mask))\n",
    "\n",
    "def inverse_boolean_mask(x_true, x_false, mask):\n",
    "    n = tf.shape(mask)[0]\n",
    "    rest = x_true.get_shape().as_list()[1:]\n",
    "    shape = [n] + rest\n",
    "\n",
    "    ind1 = tf.expand_dims(gather_indices(mask), 1)\n",
    "    ind2 = tf.expand_dims(gather_indices(tf.logical_not(mask)), 1)\n",
    "\n",
    "    z = tf.scatter_nd(ind1, x_true, shape) + tf.scatter_nd(ind2, x_false, shape)\n",
    "    return z\n",
    "\n",
    "def gather_indices(mask):\n",
    "    n = tf.shape(mask)[0]\n",
    "    return tf.boolean_mask(tf.range(n), mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no aux\n",
      "no aux\n",
      "no aux\n",
      "no aux\n",
      "no aux\n",
      "no aux\n",
      "no aux\n",
      "no aux\n",
      "no aux\n",
      "no aux\n",
      "no aux\n",
      "no aux\n",
      "no aux\n",
      "no aux\n",
      "no aux\n",
      "no aux\n",
      "no aux\n",
      "no aux\n",
      "no aux\n",
      "no aux\n",
      "no aux\n",
      "no aux\n",
      "no aux\n",
      "no aux\n",
      "no aux\n",
      "no aux\n",
      "no aux\n",
      "no aux\n",
      "no aux\n",
      "no aux\n",
      "no aux\n",
      "no aux\n",
      "no aux\n",
      "no aux\n",
      "no aux\n",
      "no aux\n",
      "no aux\n",
      "no aux\n",
      "no aux\n",
      "no aux\n"
     ]
    }
   ],
   "source": [
    "loss_name = 'inv'\n",
    "x_dim = 5\n",
    "lr = tf.placeholder(tf.float32, shape=())\n",
    "g = Gaussian(np.zeros((5,)), np.diag([100, 0.1, 10, 1.0, 0.01]))\n",
    "# g = GaussianFunnel(dim=2, clip=12.)\n",
    "# M = get_rotation(np.pi / 4)\n",
    "# mu = np.zeros(2,)\n",
    "# sigma = np.array([[100, 0.0], [0., 0.1]])\n",
    "\n",
    "# g = Gaussian(mu, M.dot(sigma).dot(M.T))\n",
    "\n",
    "dynamics = Dynamics(x_dim, g.get_energy_function(), T=5, eps=0.1, hmc=False, eps_trainable=True, net_factory=net_factory, use_temperature=False)\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(None, x_dim))\n",
    "z = tf.random_uniform(tf.shape(x))\n",
    "\n",
    "mask = tf.cast(tf.random_uniform((tf.shape(x)[0],), maxval=2, dtype=tf.int32), tf.bool)\n",
    "x1, x2 = boolean_mask(x, mask)\n",
    "z1, z2 = boolean_mask(z, mask)\n",
    "\n",
    "Lx1, _, px1 = dynamics.forward(x1)\n",
    "Lx2, _, px2 = dynamics.backward(x2)\n",
    "\n",
    "Lz1, _, pz1 = dynamics.forward(z1)\n",
    "Lz2, _, pz2 = dynamics.forward(z2)\n",
    "\n",
    "Lx = inverse_boolean_mask(Lx1, Lx2, mask)\n",
    "Lz = inverse_boolean_mask(Lz1, Lz2, mask)\n",
    "px = inverse_boolean_mask(px1, px2, mask)\n",
    "pz = inverse_boolean_mask(pz1, pz2, mask)\n",
    "\n",
    "loss = 0.\n",
    "\n",
    "v1 = (tf.reduce_sum(tf.square(x - Lx), axis=1) * px) + 1e-4\n",
    "v2 = (tf.reduce_sum(tf.square(z - Lz), axis=1) * pz) + 1e-4\n",
    "scale = 1.0\n",
    "\n",
    "loss += scale * (tf.reduce_mean(1.0 / v1) + tf.reduce_mean(1.0 / v2))\n",
    "loss += (- tf.reduce_mean(v1) - tf.reduce_mean(v2)) / scale\n",
    "\n",
    "global_step = tf.Variable(0., name='global_step', trainable=False)\n",
    "\n",
    "learning_rate = tf.train.exponential_decay(lr, global_step,\n",
    "                                           250, 0.96, staircase=True)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "def hack_grads(g, w):\n",
    "    return tf.where(tf.is_finite(g), g, w)\n",
    "\n",
    "train_op = optimizer.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_steps = 10000\n",
    "\n",
    "noise_to_samples = []\n",
    "sample_list = []\n",
    "\n",
    "samples = np.random.randn(n_samples, x_dim)\n",
    "sample_list.append(np.copy(samples))\n",
    "\n",
    "sample_acc_prob = []\n",
    "noise_acc_prob = []\n",
    "\n",
    "loss_values = []\n",
    "numbers_list = []\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "temp = 1.0\n",
    "\n",
    "for t in range(nb_steps):\n",
    "#   if 0 <= t < 1000:\n",
    "#     temp = 5.\n",
    "#   elif 1000 <= t < 2000:\n",
    "#     temp = 5. / 2\n",
    "#   elif 2000 <= t < 3000:\n",
    "#     temp = 5. / 4\n",
    "#   else:\n",
    "#     temp = 1.0\n",
    "\n",
    "  if t % 100 == 0:\n",
    "    temp *= 0.9\n",
    "  \n",
    "  if temp <= 1.0:\n",
    "    temp = 1.0\n",
    "  \n",
    "  mask = np.random.randint(low=0, high=2, size=(n_samples,))\n",
    "  samples_forward = samples[mask == 0]\n",
    "  samples_backward = samples[mask == 1]\n",
    "\n",
    "  feed_dict={\n",
    "      x_to_forward: samples_forward,\n",
    "      x_to_backward: samples_backward,\n",
    "      s.temperature: temp,\n",
    "      lr: 1e-3,\n",
    "  }\n",
    "\n",
    "  _, loss_, x_prev, x_prop, p_x_, z_prev, z_prop, p_z_, numbers_ = sess.run([\n",
    "      train_op,\n",
    "      loss,\n",
    "      x_previous,\n",
    "      x_proposed,\n",
    "      p_x,\n",
    "      z_previous,\n",
    "      z_proposed,\n",
    "      p_z,\n",
    "      numbers\n",
    "  ], feed_dict)\n",
    "  \n",
    "  numbers_list.append(numbers_)\n",
    "  sample_acc_prob.append(np.mean(p_x_))\n",
    "  noise_acc_prob.append(np.mean(p_z_))\n",
    "\n",
    "  samples = accept(x_prev, x_prop, p_x_)\n",
    "  sample_list.append(np.copy(samples))\n",
    "  noise_to_sample = accept(z_prev, z_prop, p_z_)\n",
    "  noise_to_samples.append(np.copy(noise_to_sample))\n",
    "  loss_values.append(loss_)\n",
    "  \n",
    "  if t % 100 == 0:\n",
    "    print 'Step: %d / %d, Loss: %.2e, Acceptance sample: %.2f, Acceptance Noise: %.2f, Epsilon: %.3f, LR: %.5f' % (t, nb_steps, loss_, np.mean(p_x_), np.mean(p_z_), sess.run(s.eps), sess.run(learning_rate, {lr: 1e-3}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  # post training sampling\n",
    "  \n",
    "  # samples = g.get_samples(n=n_samples)\n",
    "  samples = np.random.randn(n_samples, 2)\n",
    "  final_samples = []\n",
    "  \n",
    "  for t in range(200):\n",
    "    final_samples.append(np.copy(samples))\n",
    "    mask = np.random.randint(low=0, high=2, size=(n_samples,))\n",
    "    samples_forward = samples[mask == 0]\n",
    "    samples_backward = samples[mask == 1]\n",
    "    \n",
    "    feed_dict = {\n",
    "        x_to_forward: samples_forward,\n",
    "        x_to_backward: samples_backward,\n",
    "        s.temperature: 1.,\n",
    "    }\n",
    "    \n",
    "    x_f_prop, x_b_prop, p_forward, p_backward = sess.run([x_forward_proposed, x_backward_proposed, p_x_forward, p_x_backward], feed_dict)\n",
    "        \n",
    "    new_samples = np.zeros((n_samples, x_dim))\n",
    "    p_accept = np.zeros((n_samples,))\n",
    "    \n",
    "    new_samples[mask == 0] = x_f_prop\n",
    "    new_samples[mask == 1] = x_b_prop\n",
    "    \n",
    "    p_accept[mask == 0] = p_forward\n",
    "    p_accept[mask == 1] = p_backward\n",
    "    \n",
    "    samples = accept(samples, new_samples, p_accept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
